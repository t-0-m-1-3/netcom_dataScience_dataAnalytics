{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montreal Python 69 - Tutorial\n",
    "### By Abbas Taher\n",
    "### GoFlek Inc.\n",
    "### Monday Feb 5th 2018\n",
    "### Three Methods to Aggregate Subscriber's Interest - Aggregating Data\n",
    "### AGENDA:\n",
    "\n",
    ">      a- Recipe 1- Using Python Dictionary\n",
    "   \n",
    ">      b- Short Overview of PySpark\n",
    "      \n",
    ">      c- Recipe 2- Using Apache Spark - GroupBy Transformation\n",
    "      \n",
    ">      d- Recipe 3- Using Apache Spark - ReduceBy Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statment: \"Aggregate Interest by ID\"\n",
    ">     For Subscribers of an Online Magazine\n",
    ">     Aggregate each subscriber's interest/likes into a (key,value) pair in one record\n",
    "\n",
    "\n",
    "### Original Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3cc4b0448aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change preview settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/InterestData.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubscrib_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',15)  # change preview settings\n",
    "\n",
    "fname = '../data/InterestData.csv'\n",
    "subscrib_data = pd.read_csv(fname, delimiter =';')\n",
    "subscrib_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output after Aggregating by ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',20)  # change preview settings\n",
    "\n",
    "fname = '../data/InterestAggregated.csv'\n",
    "subscrib_data = pd.read_csv(fname, delimiter =';')\n",
    "subscrib_data\n",
    "\n",
    "# Note: key = ID ; Interest = Long string with commas between each interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECIPE \\#1: Using A Python Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/InterestData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f97ba9cfb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read the file into a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/InterestData.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mreadCSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadCSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/InterestData.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict  \n",
    "\n",
    "aggInterest = defaultdict(list)        # dictionary <key,[list]>\n",
    "header = None\n",
    "\n",
    "# Read the file into a dictionary\n",
    "with open('./data/InterestData.csv','r') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=';')\n",
    "    header = next(readCSV)  \n",
    "    for row in readCSV:                         # loop over all records in file\n",
    "        aggInterest[row[0]].append((row[1]))    # <--- aggregating interest into the corresponding dictionary key\n",
    "\n",
    "# Write the dictionary data into the file         \n",
    "with open('./data/aggInterest.csv', 'w') as csvfile:         \n",
    "    writeCSV = csv.writer(csvfile, delimiter=';')\n",
    "    writeCSV.writerow(header)\n",
    "    for idd, interest in aggInterest.items():\n",
    "        writeCSV.writerow((idd,interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('./data/aggInterest.csv', 'r')\n",
    "for line in text_file:\n",
    "    print(line)\n",
    "    \n",
    "#Note: This is not quite what we are looking for, we want to save a long string as one field rather than a list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/aggInterest.csv', 'w') as csvfile:         \n",
    "    writeCSV = csv.writer(csvfile, delimiter=';')\n",
    "    writeCSV.writerow(header)\n",
    "    for idd, interestLst in aggInterest.items():\n",
    "        interest = \",\".join(interestLst)        #  <--- join the list into a long string\n",
    "        writeCSV.writerow((idd,interest))\n",
    "\n",
    "text_file = open('../data/aggInterest.csv', 'r')\n",
    "for line in text_file:\n",
    "    print(line)\n",
    "\n",
    "# Note: This is better ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Overview of PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename = \"PySpark_Execution_Process.png\", width=750, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename =\"PySpark_Execution_Architecture.png\", width=800, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECIPE \\#2: Using Apache Spark - GroupBy Transformation\n",
    "### 1- Read Subsscriber Interest only data \n",
    "### 2- Use groupByKey to merge interest \n",
    "### 3- Convert the group-by list into a long string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename =\"Using_GroupByKey.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.master\", \"local\").appName(\"PythonCSV\").getOrCreate()\n",
    "\n",
    "file = \"../data/InterestData.csv\"\n",
    "\n",
    "# read csv file includes header into a dataframe\n",
    "dataframe = spark.read.csv(file, sep=\";\", inferSchema=\"true\", header=True)\n",
    "\n",
    "rdd1 = dataframe.rdd.groupByKey()\n",
    "\n",
    "# to convert groupByKey() iterable into an actual list of strings (interest)\n",
    "rdd2 = rdd1.map(lambda pair : (pair[0], list(pair[1])))\n",
    "\n",
    "\n",
    "rdd3 = rdd2.map(lambda pair: (pair[0], (\",\".join(pair[1])) )) # convert list -> long string\n",
    "\n",
    "for row in rdd3.collect():\n",
    "    print(row)\n",
    "\n",
    "print('\\n',rdd1.collect()[0])  # just to show that the groupBy returned an iterator\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECIPE \\#3: Using Apache Spark - ReduceBy Transformation\n",
    "### 1- Read Subsscriber Interest only data \n",
    "### 2- Use ReduceByKey to merge interest \n",
    "### 3- Convert the interest value list into a long string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename =\"Using_ReduceByKey.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.master\", \"local\").appName(\"PythonCSV\").getOrCreate()\n",
    "\n",
    "file = \"../data/InterestData.csv\"\n",
    "\n",
    "# read csv file includes header into a dataframe\n",
    "dataframe = spark.read.csv(file, sep=\";\", inferSchema=\"true\", header=True)\n",
    "\n",
    "# Convert value part -> Set('Interest') using mapValues\n",
    "# Make a union of all Sets belonging to same key using reduceByKey\n",
    "rdd1 = dataframe.rdd.mapValues(lambda interest: {interest})\\\n",
    "                .reduceByKey(lambda s1, s2: s1.union(s2))  \n",
    "\n",
    "\n",
    "rdd3 = rdd1.mapValues(lambda sett: ((\",\".join(sett)) )) # convert value part -> long string\n",
    "\n",
    "for row in rdd3.collect():\n",
    "    print(row)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "### Using reduceByKey is better than GroupBy because of the smaller shuffle size. When using redyceByKey  some aggregation occurs inside the Spark cluster where as in GroupByKey some processing happens in PySpark driver at the client side.\n",
    "\n",
    "### Using Set {} in reduceByKey is better because it eleminate duplicate in (ID,Interest) pairs if there are any in InterestData.csv file.\n",
    "\n",
    "### Using mapValues is better than using map when the key stays the same after a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## https://github.com/abbas-taher/Montreal-Python-69"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
